# 模型信息收集

### 模型列表
| 分类 | 名称 | 功能 | 地址 | 教程 |
| --- | --- | --- | --- | --- |
| 音频 | silero-vad | 音频VAD           | https://github.com/snakers4/silero-vad | https://github.com/snakers4/silero-vad/README.md |
| 音频 | XTTS       | 文本转语音，声音克隆 | https://github.com/coqui-ai/TTS | https://docs.coqui.ai/en/latest/index.html |
| 音频 | FunASR | 语音识别，语音转文字 | https://github.com/modelscope/FunASR | https://github.com/modelscope/FunASR/tree/main/runtime/docs |
| 音频 | iic/speech_fsmn_vad_zh-cn-16k-common-onnx | 音频VAD | https://www.modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-onnx/summary | runtime/onnxruntime/bin/funasr-onnx-online-vad.cpp |
| 文本 | thuduj12/fst_itn_zh | 逆文本正则化，中文 | https://www.modelscope.cn/models/thuduj12/fst_itn_zh/ | https://github.com/wenet-e2e/WeTextProcessing <br> https://mp.weixin.qq.com/s/q_11lck78qcjylHCi6wVsQ | 

## RAG
[基于RAG的大模型知识库构建指南-知乎](https://zhuanlan.zhihu.com/p/24923829948)

### bge-multilingual-gemma2
https://huggingface.co/BAAI/bge-multilingual-gemma2

## DeepSeek
### DeepSeek-R1-Distill-Qwen-32B-AWQ
下载```huggingface-cli download --resume-download Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ --local-dir /mnt/ext_file/deepseek-r1-distill-qwen-32b-awq```
资源
| 静态显存 | 运行时显存 | Avg generation throughput |
| ------  | --------- | ---- |
| model weights took 18.1678 GB  | 总显存 * gpu-memory-utilization  | nvidia 3090 (awq): 30 tokens/s <br> nvidia 3090 (awq_marlin): 36 tokens/s |
### 使用vllm运行模型
启动openai http服务
```python -m vllm.entrypoints.openai.api_server --model /mnt/ext_file/deepseek-r1-distill-qwen-32b-awq --quantization awq_marlin --tensor-parallel-size 1 --max_model_len 3000 --max_seq_len 3000 --gpu-memory-utilization 0.95 --enforce-eager ```
使用curl测试其openai的api
```
curl http://localhost:8000/v1/models
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/mnt/ext_file/deepseek-r1-distill-qwen-32b-awq",
        "prompt": "编写一个C++程序，输出数组中最大的值",
        "max_tokens": 7,
        "temperature": 0
    }'
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/mnt/ext_file/deepseek-r1-distill-qwen-32b-awq",
        "messages": [
            {"role": "system", "content": "你是一个程序员助手。"},
            {"role": "user", "content": "编写一个c++程序，计算第N个斐波那契数"}
        ]
    }'

```

## 音频、数字人
### TTS XTTS
https://github.com/coqui-ai/TTS -- 这个是比较老的代码库，已经基本不维护了。
https://huggingface.co/coqui/XTTS-v2/tree/main
https://github.com/coqui-ai/xtts-streaming-server.git -- 用于测试XTTS的HTTP API服务，很有参考价值以及很好用。
#### 安装
推荐使用代码安装。
```
git clone https://github.com/coqui-ai/TTS && cd TTS
make system-deps
pip install -e .[all] -i https://pypi.mirrors.ustc.edu.cn/simple/
```
#### 测试方法1：python代码直接执行模型推理进行测试
```
import torch
from TTS.api import TTS
device = "cuda" if torch.cuda.is_available() else "cpu"
print(TTS().list_models())
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
tts.tts_to_file(text="这是第一句的语音，感谢xtts", 
                speaker_wav="/home/zhuqingquan/data_for_test/TextTTSFromCAO.wav", 
                language="zh-cn", 
                file_path="/home/zhuqingquan/tts_out.wav")
```
#### 测试方法2：部署xtts-streaming-server服务
可以查看README.md文件看如何安装python依赖库。如果之前已经安装TTS库，则可以这里把requirements.txt里面的TTS依赖注释掉。
运行API服务，使用了fastapi。
```
cd server
uvicorn main:app --host 0.0.0.0 --port 8088
```

### TTS XTTS from idiap
https://github.com/idiap/coqui-ai-TTS -- 这个是从coqui TTS项目中fork出来的，并且持续有人在维护
安装方式与前面的coqui-tts的一样。安装完后可以通过list_models进行验证。
```
python TTS/server/server.py --list_models
```
启动TTS服务
```
CUDA_VISIBLE_DEVICES="0" python TTS/server/server.py     --model_name tts_models/multilingual/multi-dataset/xtts_v2 --use_cuda
```

### ASR之FunASR
[FunASR-github](https://github.com/modelscope/FunASR)
FunASR除了asr模型之后，还提供了一系列声音相关的模型。including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR.
#### 从代码安装
```
git clone https://github.com/alibaba/FunASR.git && cd FunASR
conda create --name asr python=3.12.9
pip3 install -e ./
pip3 install torch=2.6.0
pip3 install torchaudio=2.6.0
funasr ++model=paraformer-zh ++vad_model="fsmn-vad" ++punc_model="ct-punc" ++input=/home/zhuqingquan/data_for_test/mairuo-1.wav
# 执行过程中将会使用modelscope将模型下载到本地路径/home/zhuqingquan/.cache/modelscope/hub/models中
```
#### 使用onnxruntime c++运行ASR推理
具体查看runtime/onnxruntime/readme.md文件里面的指引。

#### 例子
在FunASR/examples/README_zh.md中有多个例子的代码，可以用于测试一些独立的功能。

### ASR之Whisper
### ASR其他
[PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech?tab=readme-ov-file)
[mms-Massively Multilingual Speech](https://github.com/facebookresearch/fairseq/blob/main/examples/mms/README.md)

### ClearVoice-Studio
[github项目地址](https://github.com/modelscope/ClearerVoice-Studio.git)

ClearVoice内包含多个音频模型，覆盖以下音频处理任务：
| Tasks (Sampling rate) | Models (HuggingFace Links)|
|-------|--------------------------|
|Speech Enhancement (16kHz & 48kHz)| `MossFormer2_SE_48K` ([HuggingFace](https://huggingface.co/alibabasglab/MossFormer2_SE_48K)), `FRCRN_SE_16K` ([HuggingFace](https://huggingface.co/alibabasglab/FRCRN_SE_16K)), `MossFormerGAN_SE_16K` ([HuggingFace](https://huggingface.co/alibabasglab/MossFormerGAN_SE_16K)) | 
|Speech Separation (16kHz)|`MossFormer2_SS_16K` ([HuggingFace](https://huggingface.co/alibabasglab/MossFormer2_SS_16K))|
|Speech Super-Resolution (48kHz)|`MossFormer2_SR_48K`([HuggingFace](https://huggingface.co/alibabasglab/MossFormer2_SR_48K))|
|Audio-Visual Target Speaker Extraction (16kHz)|`AV_MossFormer2_TSE_16K` ([HuggingFace](https://huggingface.co/alibabasglab/AV_MossFormer2_TSE_16K))|

## 多模态、视频
### SmolVLM
[模型地址-huggingface](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct)
```
huggingface-cli download HuggingFaceTB/SmolVLM-Instruct --local-dir ./model/
# 为了方便后面安装flash-attention-2，此处torch选择2.6版本
pip install torch==2.6 transformers -i https://mirrors.aliyun.com/pypi/simple/
# 如果需要运行量化后的模型，还需要安装一下依赖
pip install 'accelerate>=0.26.0'
pip install -U bitsandbytes
```

未验证测试的开源模型
人脸识别、人脸检测
+ [Ultra-Light-Fast-Generic-Face-Detector-1MB](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB)
这个模型只检测人脸，人脸的关键点无法检测

[TTS模型paper汇总](https://github.com/coqui-ai/TTS-papers)
[libfacedetection](https://github.com/ShiqiYu/libfacedetection/)
[RFBNet](https://github.com/GOATmessi8/RFBNet)
[deepinsight/insightface](https://github.com/deepinsight/insightface)
[retinaface](https://github.com/ternaus/retinaface?tab=readme-ov-file)
[RFSong-7993](https://github.com/songwsx/RFSong-7993)
[pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd)