### 编程实现过程
1. 分析神经网络应该采用什么结构。主要涉及以下方面：
> * 问题域的内容如何处理。每个神经元处理的数据是什么。
> * 神经网络中的层之间的权重weight如何定义。
> * 神经网络层之间的阈值如何定义。
2. 初始化神经网络

### transformer
#### 开源实现代码
[annotated-transformer](https://github.com/harvardnlp/annotated-transformer)

### 资源
+ fastai -- AI库，推荐作为学习使用。包含很多的内容。
+ ImageNet -- 图片数据库，用于训练模型
+ 模型resnet18 -- 图像识别，用ImageNet进行训练

### huggingface接入
添加国内镜像站，具体可访问 https://hf-mirror.com/
配置HF_HOME、HF_ENDPOINT
```
export HF_HOME=/mnt/ext_file/huggingface_home
export HF_ENDPOINT=https://hf-mirror.com      
```
安装huggingface_hub进行模型和数据的下载缓存
```
pip install -U huggingface_hub
# --local-dir会指定将模型下载到特定的路径下
huggingface-cli download facebook/opt-125m --local-dir facebook-opt-125m
# 下载数据集
huggingface-cli download --repo-type dataset --resume-download wikitext --local-dir wikitext
```

### modelscope下载模型
配置MODELSCOPE_CACHE设置下载模型的缓存路径，比如
```
export MODELSCOPE_CACHE=/media/zhuqingquan/project/code/models-cache/modelscope
```
安装modelscope sdk
```
pip install modelscope
```

### 安装flash-attention-2
#### 手动下载安装
[参考](https://zhuanlan.zhihu.com/p/1897638897388873200)
```
python --version &&
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())" &&
nvcc -V
```
- 下载flash-attention-2的wheel包时，cuda的版本以`nvcc -v`输出的版本为准
- 在这里选择匹配本地环境的whl进行下载，建议下载cxx11abiFALSE的whl，如果下载cxx11abiTRUE，可能后续使用时会报错:
    `#/opt/conda/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE`
    这里并不是根据`python -c "import torch;print(torch._C._GLIBCXX_USE_CXX11_ABI)"`输出的结果来选择TRUE或者FALSE。

### 端侧部署运行推理模型
#### QNN
端侧部署步骤：
模型转换-->将bin+cpp编译成so动态库-->在目标机器上动态加载so进行推理
##### 安装
[官方教程--安装QNN SDK以及开放环境](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/windows_setup.html?product=1601111740009302)
[supported-snapdragon-devices](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/overview.html?product=1601111740009302#tbl-supported-snapdragon-devices)
注意：
1. 当需要下载Hexagon Tools或者Hexagon SDK时，教程中推荐下载QPM3先。但是QPM3安装后一直无法登录。推荐是下载Qualcomm_Software_Center，这个比较靠谱。
2. 在Qualcomm Software Center中搜索下载Qualcomm® Hexagon™ Toolchain。

##### 在Windows主机上进行模型转换与部署 
[各种模型转换工具的使用说明](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tools.html)
[官方模型转换教程](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/qnn_tutorial_linux_host.html?product=1601111740009302)
```
Set-ExecutionPolicy -ExecutionPolicy ByPass -Scope CurrentUser
C:\ProgramData\anaconda3\shell\condabin\conda-hook.ps1
conda create -n "qnn-sdk" python=3.10
conda activate qnn-sdk

 [System.Environment]::SetEnvironmentVariable('QNN_SDK_ROOT', "D:\\3rdlib\\qairt\\2.36.0.250627", 'User');
 $env:QNN_SDK_ROOT = [System.Environment]::GetEnvironmentVariable('QNN_SDK_ROOT', 'User');
```
Tensorflow模型转换成QNN，例子是InspectionV3
**注意：如果是在$env:QNN_SDK_ROOT中运行下面命令，可能报错：`ModuleNotFoundError: No module named 'qti'`。此时可以在文件`qnn-tensorflow-converter`中开头那里加一行`sys.path.append('lib/python')`**
因为我们当前进行模型转换和生成模型dll的系统是Windows，所以选择bin\x86_64-windows-msvc下面的工具进行操作。
```shell
python $env:QNN_SDK_ROOT\bin\x86_64-windows-msvc\qnn-tensorflow-converter `
   --input_network "$env:QNN_SDK_ROOT/examples/Models/InceptionV3/tensorflow/inception_v3_2016_08_28_frozen.pb" `
   --input_dim input 1,299,299,3 `
   --out_node InceptionV3/Predictions/Reshape_1 `
   --output_path "$env:QNN_SDK_ROOT/examples/Models/InceptionV3/model/Inception_v3.cpp"

# 加--input_list参数进行量化
python $env:QNN_SDK_ROOT\bin\x86_64-windows-msvc\qnn-tensorflow-converter `
   --input_network "$env:QNN_SDK_ROOT/examples/Models/InceptionV3/tensorflow/inception_v3_2016_08_28_frozen.pb" `
   --input_dim input 1,299,299,3 `
   --input_list "$env:QNN_SDK_ROOT/examples/Models/InceptionV3/data/cropped/raw_list.txt" `
   --out_node InceptionV3/Predictions/Reshape_1 `  
   --output_path "$env:QNN_SDK_ROOT/examples/Models/InceptionV3/model/quantized/Inception_v3.cpp"
```
qnn-tensorflow-converter转换后的bin文件，真实格式是`.tar`，内容是未压缩的weights和biase数据。cpp文件则包含了如何调用QNN API构建模型graph的代码。
将cpp+bin文件编译成so。编译过程中会使用`bin\\x86_64-windows-msvc\\object-generator`将解压后的raw数据转换成c语音的.o文件（这个过程就是`_extract_obj_from_bin`）。

**input_list的格式**
--input_list - This argument provides a file containing paths to input files to be used for graph execution. Input files can be specified with the below format:
```
<input_layer_name>:=<input_layer_path>[<space><input_layer_name>:=<input_layer_path>]
[<input_layer_name>:=<input_layer_path>[<space><input_layer_name>:=<input_layer_path>]]
...
```
Below is an example containing 3 sets of inputs with layer names “Input_1” and “Input_2”, and files located in the relative path “Placeholder_1/real_input_inputs_1/”:
```
Input_1:=Placeholder_1/real_input_inputs_1/0-0#e6fb51.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/0-1#8a171b.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/1-0#67c965.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/1-1#54f1ff.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/2-0#b42dc6.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/2-1#346a0e.rawtensor
```

```shell
python "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" `
    -c "Inception_v3.cpp" `
    -b "Inception_v3.bin" `
    -o model_libs `
    -t ${QNN_TARGET_ARCH}

```
如果成功会构建出来dll或者so，动态库会导出函数`QnnModel_composeGraphs`以及`QnnModel_freeGraphsInfo`。

vscode launch.json用于调试
```json
// qnn-onnx-converter 将onnx模型转换成QNN的cpp+bin文件
{
    "name": "qnn_onnx_converter",
    "type": "debugpy",
    "request": "launch",
    //"program": "${file}",
    "program": "${workspaceFolder}/bin/x86_64-linux-clang/qnn-onnx-converter",
    "cwd": "${workspaceFolder}",
    "console": "integratedTerminal",
    "args": [
        "--input_network",
        "data_for_test/android/23745_onnx/model_name.onnx",
        "--input_dim",
        "input1",
        "8,1,256,256",
        "--input_dim",
        "input2",
        "8,3,256,256",
        "--output_path",
        "data_for_test/android/23745_qnn_model/model_name.cpp",
    ]
},
// qnn-model-lib-generator 将converter生成的cpp+bin文件生成so动态库文件
{
    "name": "qnn-model-lib-generator",
    "type": "debugpy",
    "request": "launch",
    //"program": "${file}",
    "program": "${workspaceFolder}/bin/x86_64-linux-clang/qnn-model-lib-generator",
    "cwd": "${workspaceFolder}",
    "console": "integratedTerminal",
    "args": [
        "-c",
        "data_for_test/android/23745_qnn_model/model_name.cpp",
        "-b",
        "data_for_testk/android/23745_qnn_model/model_name.bin",
        "-t",
        "aarch64-android",
        "-o",
        "data_for_test/android/23745_qnn_model/",
    ]
}
```

##### 部署到Android手机中运行
1. 在App的AndroidManifest.xml中添加native library的访问权限。
```
    # libcdsprpc.so是必须的，其他2个不确定

    <application>
        <uses-native-library android:name="libsdsprpc.so" android:required="false" />
        <uses-native-library android:name="libcdsprpc.so" android:required="false" />
        <uses-native-library android:name="libadsprpc.so" android:required="false" />
    </application>
```

##### QnnDevice_Create失败的场景
1. 因加载不到与Soc对应的Stub.so而导致的失败。比如加载不到libQnnHtpV69Stub.so。
logcat中可以查看到类似的错误信息：`QnnDsp <W> Failed in loading stub: dlopen failed: library "libQnnHtpV69Stub.so" not found`。
解决方式：在打包或者构建后需要将libQnnHtpVxxStub.so也拷贝过去。为了适配更广泛的设备可能需要多拷贝几个Soc版本的Stub.so。

2. 因加载不到`libcdsprpc.so`而导致的失败。
logcat中可以查看到类似的错误信息：`QnnDsp <W> Failed in loading stub: dlopen failed: library "libcdsprpc.so" not found: needed by /data/app/.../lib/arm64-v8a/libQnnHtpV69Stub.so`
解决方式：安卓应该要在App的AndroidManifest.xml中添加`<use-native-library>`。

3. 版本还是权限问题？
```
vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:2989: Error 0xd: open_shell failed for domain 3 search paths used are /dsp/, /vendor/dsp/, /vendor/dsp/xdsp/ (errno Permission denied)
vendor/qcom/proprietary/adsprpc/src/log_config.c:574:Error : Unable to add watcher for folder /system/vendor/lib/rfsa/adsp : errno is Permission denied
vendor/qcom/proprietary/adsprpc/src/log_config.c:574:Error : Unable to add watcher for folder /vendor/lib/rfsa/adsp : errno is Permission denied
vendor/qcom/proprietary/adsprpc/src/apps_std_imp.c:880: Successfully opened file libQnnHtpV69Skel.so

QnnDsp <E> Failed to retrieve skel build id: err: 1
QnnDsp <E> Error in verify skel version
```
基于网络上面其他人的处理方式，猜测是QNN SDK版本与手机内的skel版本不一致导致的问题。
可以降版本试试。

##### QnnDevice_Create的流程
**以下内容是根据输出信息猜测的，并没有去QNN的技术文档中确认，未必准确。**
1. 用户调用libQnnHtp.so的QnnDevice_Create时，会尝试动态加载与Soc芯片对应的stub.so动态库。比如骁龙8 gen1对应的Hexagon Arch是v69，就会尝试动态加载libQnnHtpV69Stub.so。
2. 在libQnnHtpV69Stub.so中会尝试动态加载native library: libcdsprpc.so。
3. 在libcdsprpc.so中会尝试与dspservice建立rpc连接。可以使用以下adb命令判断手机是否启动了dspservice。没启动的手机不支持DSP。
```adb shell ps -A | grep -E 'dsp'```
4. libcdsprpc.so如果与dspservice成功建立连接。libQnnHtpVxxStub.so会使用这个rpc连接调用libQnnHtpVxxSkel.so中的实现。
libQnnHtpVxxSkel.so应该是被dspservice动态加载起来并调用。

##### 参考文档
[DSP Runtime Environment](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/dsp_runtime.html)

#### SNPE (Nueral Processing Engin)
[SNPECam-github](https://github.com/fdchiu/SNPECam)

#### 量化
高通的各种backend对应不同的量化要求
Choosing Between a Quantized or Non-Quantized Model
+ CPU - Choose a non-quantized model. Quantized models are currently incompatible with the CPU backend.
+ DSP - Choose a quantized model. Quantized models are required when running on the DSP backend.
+ GPU - Choose a non-quantized model. Quantized models are currently incompatible with the GPU backend.
+ HTP - Choose a quantized model. Quantized models are required when running on the HTP backend.
+ HTA - Choose a quantized model. Quantized models are required when running on the HTA backend.

#### 手机型号以及对应的NPU芯片信息
| 手机型号 | 芯片 | GPU | NPU | Hexagon Arch | 手机发布时间 |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 小米12 pro | 高通 骁龙8 Gen1 | 高通 Adreno730 | 第七代AI芯片 | V69 | 2021年12月 |